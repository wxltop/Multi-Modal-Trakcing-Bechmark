# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import math
import torch.nn.functional as F
from torch import nn
import ltr.models.backbone.resnet as resnet
import ltr.models.backbone.resnext_dconv_mrcnn as resnext_mrcnn

from collections import OrderedDict
from ltr.models.backbone.base import Backbone
from ltr.models.backbone.resnext_dconv_mrcnn import get_norm


class LastLevelMaxPool(nn.Module):
    """
    This module is used in the original FPN to generate a downsampled
    P6 feature from P5.
    """

    def __init__(self):
        super().__init__()
        self.num_levels = 1
        self.in_feature = "p5"

    def forward(self, x):
        return [F.max_pool2d(x, kernel_size=1, stride=2, padding=0)]


class FPN(Backbone):
    """
    This module implements Feature Pyramid Network.
    It creates pyramid features built on top of some input feature maps.
    """

    def __init__(
        self, bottom_up, in_features, out_channels, fuse_type="sum", output_layers='default', top_block=None,
            frozen_layers=(), norm=None
    ):
        """
        Args:
            bottom_up (Backbone): module representing the bottom up subnetwork.
                Must be a subclass of :class:`Backbone`. The multi-scale feature
                maps generated by the bottom up network, and listed in `in_features`,
                are used to generate FPN levels.
            in_features (list[str]): names of the input feature maps coming
                from the backbone to which FPN is attached. For example, if the
                backbone produces ["res2", "res3", "res4"], any *contiguous* sublist
                of these may be used; order must be from high to low resolution.
            out_channels (int): number of channels in the output feature maps.
            norm (str): the normalization to use.
            top_block (nn.Module or None): if provided, an extra operation will
                be performed on the output of the last (smallest resolution)
                FPN output, and the result will extend the result list. The top_block
                further downsamples the feature map. It must have an attribute
                "num_levels", meaning the number of extra FPN levels added by
                this block, and "in_feature", which is a string representing
                its input feature (e.g., p5).
            fuse_type (str): types for fusing the top down features and the lateral
                ones. It can be "sum" (default), which sums up element-wise; or "avg",
                which takes the element-wise mean of the two.
        """
        super(FPN, self).__init__(frozen_layers=frozen_layers)
        assert isinstance(bottom_up, Backbone)

        self.output_layers = output_layers

        # Feature map strides and channels from the bottom up network (e.g. ResNet)
        in_strides = [bottom_up.out_feature_strides(f) for f in in_features]
        in_channels = [bottom_up.out_feature_channels(f) for f in in_features]

        _assert_strides_are_log2_contiguous(in_strides)
        lateral_convs = []
        output_convs = []
        output_names = []

        self.fpn = nn.Module()
        use_bias = norm is None
        for idx, in_channels in enumerate(in_channels):
            lateral_conv = nn.Conv2d(
                in_channels, out_channels, kernel_size=1, bias=use_bias)
            output_conv = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=use_bias)
            stage = int(math.log2(in_strides[idx]))
            self.fpn.add_module("fpn_lateral{}".format(stage), lateral_conv)
            self.fpn.add_module("fpn_lateral{}_norm".format(stage), get_norm(norm, out_channels))

            self.fpn.add_module("fpn_output{}".format(stage), output_conv)
            self.fpn.add_module("fpn_output{}_norm".format(stage), get_norm(norm, out_channels))

            output_names.append("fpn_output{}".format(stage))

            lateral_convs.append("fpn_lateral{}".format(stage))
            output_convs.append("fpn_output{}".format(stage))

        self.top_block = top_block

        # Place convs into top-down order (from low to high resolution)
        # to make the top-down computation in forward clearer.
        self.lateral_convs = lateral_convs[::-1]
        self.output_convs = output_convs[::-1]
        self.output_names = output_names[::-1]

        self.in_features = in_features

        # Return feature names are "p<stage>", like ["p2", "p3", ..., "p6"]
        self._out_feature_strides = {"p{}".format(int(math.log2(s))): s for s in in_strides}

        if self.top_block is not None:
            for s in range(stage, stage + self.top_block.num_levels):
                self._out_feature_strides["p{}".format(s + 1)] = 2 ** (s + 1)

        self._out_features = list(self._out_feature_strides.keys())
        self._out_feature_channels = {k: out_channels for k in self._out_features}
        self._size_divisibility = in_strides[-1]
        assert fuse_type in {"avg", "sum"}
        self._fuse_type = fuse_type

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

        # TODO better way to avoid resetting bottom_up weights?
        self.bottom_up = bottom_up

    def _add_output_and_check(self, name, x, outputs, output_layers):
        if name in output_layers:
            outputs[name] = x
        return len(output_layers) == len(outputs)

    def forward(self, x, output_layers=None):
        """
        Args:
            input (dict[str->Tensor]): mapping feature map name (e.g., "res5") to
                feature map tensor for each feature level in high to low resolution order.

        Returns:
            dict[str->Tensor]:
                mapping from feature map name to FPN feature map tensor
                in high to low resolution order. Returned feature names follow the FPN
                paper convention: "p<stage>", where stage has stride = 2 ** stage e.g.,
                ["p2", "p3", ..., "p6"].
        """
        outputs = OrderedDict()
        if output_layers is None:
            output_layers = self.output_layers

        if 'fpn_output2' in output_layers:
            backbone_output_layers = ['layer1', 'layer2', 'layer3', 'layer4']
        elif 'fpn_output3' in output_layers:
            backbone_output_layers = ['layer2', 'layer3', 'layer4']
        elif 'fpn_output4' in output_layers:
            backbone_output_layers = ['layer3', 'layer4']
        else:
            backbone_output_layers = ['layer4', ]

        # Reverse feature maps into top-down order (from low to high resolution)
        bottom_up_features = self.bottom_up(x, backbone_output_layers)

        for feat_name, feat in bottom_up_features.items():
            if self._add_output_and_check(feat_name, feat, outputs, output_layers):
                return outputs

        x = [bottom_up_features[f] for f in self.in_features[::-1] if f in bottom_up_features]
        prev_features = getattr(self.fpn, self.lateral_convs[0])(x[0])

        if hasattr(self.fpn, self.lateral_convs[0] + '_norm'):
            prev_features = getattr(self.fpn, self.lateral_convs[0] + '_norm')(prev_features)

        out_features = getattr(self.fpn, self.output_convs[0])(prev_features)

        if hasattr(self.fpn, self.output_convs[0] + '_norm'):
            out_features = getattr(self.fpn, self.output_convs[0] + '_norm')(out_features)

        if self._add_output_and_check(self.output_names[0], out_features,
                                      outputs, output_layers):
            return outputs

        for features, lateral_conv, output_conv, output_name in zip(
            x[1:], self.lateral_convs[1:], self.output_convs[1:], self.output_names[1:]
        ):
            top_down_features = F.interpolate(prev_features, scale_factor=2, mode="nearest")
            lateral_features = getattr(self.fpn, lateral_conv)(features)

            if hasattr(self.fpn, lateral_conv + '_norm'):
                lateral_features = getattr(self.fpn, lateral_conv + '_norm')(lateral_features)

            prev_features = lateral_features + top_down_features
            if self._fuse_type == "avg":
                prev_features /= 2

            out_features = getattr(self.fpn, output_conv)(prev_features)

            if hasattr(self.fpn, output_conv + '_norm'):
                out_features = getattr(self.fpn, output_conv + '_norm')(out_features)

            if self._add_output_and_check(output_name, out_features, outputs,
                                          output_layers):
                return outputs

        raise Exception('Invalid output layers')


def _assert_strides_are_log2_contiguous(strides):
    """
    Assert that each stride is 2x times its preceding stride, i.e. "contiguous in log2".
    """
    for i, stride in enumerate(strides[1:], 1):
        assert stride == 2 * strides[i - 1], "Strides {} {} are not log2 contiguous".format(
            stride, strides[i - 1]
        )


def resnext_152_32x8d_dconv_fpn(output_layers=None, frozen_layers=(), **kwargs):
    if output_layers is None:
        output_layers = ['fpn_output5', 'fpn_output4', 'fpn_output3', 'fpn_output2']
    else:
        for l in output_layers:
            if l not in ['conv1', 'layer1', 'layer2', 'layer3', 'layer4',
                         'fpn_output5', 'fpn_output4', 'fpn_output3', 'fpn_output2']:
                raise ValueError('Unknown layer: {}'.format(l))

    backbone_output_layers = ['layer1', 'layer2', 'layer3', 'layer4']

    if isinstance(frozen_layers, (list, tuple)):
        frozen_layers_backbone = [l for l in frozen_layers if l in ['conv1', 'layer1', 'layer2', 'layer3',
                                                                    'layer4']]
    else:
        frozen_layers_backbone = frozen_layers

    backbone = resnext_mrcnn.resnext_152_32x8d_dconv(backbone_output_layers, frozen_layers=frozen_layers_backbone,
                                                     **kwargs)

    if isinstance(frozen_layers, (list, tuple)):
        frozen_layers_fpn = [l for l in frozen_layers if l in ['none', 'fpn_output5', 'fpn_output4', 'fpn_output3',
                                                               'fpn_output2', 'fpn_lateral5', 'fpn_lateral4',
                                                               'fpn_lateral3', 'fpn_lateral2', 'all']]
    else:
        frozen_layers_fpn = frozen_layers

    model = FPN(backbone, ['layer1', 'layer2', 'layer3', 'layer4'], out_channels=256,
                output_layers=output_layers, top_block=LastLevelMaxPool(), frozen_layers=frozen_layers_fpn, **kwargs)

    return model


def resnet50_fpn(output_layers=None, frozen_layers=(), **kwargs):
    if output_layers is None:
        output_layers = ['fpn_output5', 'fpn_output4', 'fpn_output3', 'fpn_output2']
    else:
        for l in output_layers:
            if l not in ['conv1', 'layer1', 'layer2', 'layer3', 'layer4',
                         'fpn_output5', 'fpn_output4', 'fpn_output3', 'fpn_output2']:
                raise ValueError('Unknown layer: {}'.format(l))

    backbone_output_layers = ['layer1', 'layer2', 'layer3', 'layer4']

    if isinstance(frozen_layers, (list, tuple)):
        frozen_layers_backbone = [l for l in frozen_layers if l in ['conv1', 'layer1', 'layer2', 'layer3',
                                                                    'layer4']]
    else:
        frozen_layers_backbone = frozen_layers

    backbone = resnext_mrcnn.resnet50(backbone_output_layers, frozen_layers=frozen_layers_backbone,
                                      **kwargs)

    if isinstance(frozen_layers, (list, tuple)):
        frozen_layers_fpn = [l for l in frozen_layers if l in ['none', 'fpn_output5', 'fpn_output4', 'fpn_output3',
                                                               'fpn_output2', 'fpn_lateral5', 'fpn_lateral4',
                                                               'fpn_lateral3', 'fpn_lateral2', 'all']]
    else:
        frozen_layers_fpn = frozen_layers

    model = FPN(backbone, ['layer1', 'layer2', 'layer3', 'layer4'], out_channels=256,
                output_layers=output_layers, top_block=LastLevelMaxPool(), frozen_layers=frozen_layers_fpn, **kwargs)

    return model


def resnet50_fpn_gn(output_layers=None, frozen_layers=(), **kwargs):
    if output_layers is None:
        output_layers = ['fpn_output5', 'fpn_output4', 'fpn_output3', 'fpn_output2']
    else:
        for l in output_layers:
            if l not in ['conv1', 'layer1', 'layer2', 'layer3', 'layer4',
                         'fpn_output5', 'fpn_output4', 'fpn_output3', 'fpn_output2']:
                raise ValueError('Unknown layer: {}'.format(l))

    backbone_output_layers = ['layer1', 'layer2', 'layer3', 'layer4']

    if isinstance(frozen_layers, (list, tuple)):
        frozen_layers_backbone = [l for l in frozen_layers if l in ['conv1', 'layer1', 'layer2', 'layer3',
                                                                    'layer4']]
    else:
        frozen_layers_backbone = frozen_layers

    backbone = resnext_mrcnn.resnet50_gn(backbone_output_layers, frozen_layers=frozen_layers_backbone,
                                         **kwargs)

    if isinstance(frozen_layers, (list, tuple)):
        frozen_layers_fpn = [l for l in frozen_layers if l in ['none', 'fpn_output5', 'fpn_output4', 'fpn_output3',
                                                               'fpn_output2', 'fpn_lateral5', 'fpn_lateral4',
                                                               'fpn_lateral3', 'fpn_lateral2', 'all']]
    else:
        frozen_layers_fpn = frozen_layers

    model = FPN(backbone, ['layer1', 'layer2', 'layer3', 'layer4'], out_channels=256,
                output_layers=output_layers, top_block=LastLevelMaxPool(), frozen_layers=frozen_layers_fpn, norm='gn',
                **kwargs)

    return model